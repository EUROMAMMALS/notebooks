{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sudden-monaco",
   "metadata": {},
   "source": [
    "# NDVI for Sentinel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-vertex",
   "metadata": {},
   "source": [
    "The following code is able to extract NDVI data from Sentinel scenes using the one of the area with data available between two dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-casino",
   "metadata": {},
   "source": [
    "First we load the needed Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "import pystac_client\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import hvplot.pandas\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import shapely.geometry\n",
    "from ipyleaflet import Map, TileLayer, GeoJSON, FullScreenControl\n",
    "import stackstac\n",
    "from geogif import gif, dgif\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.rio.sample import sample\n",
    "import pyproj\n",
    "from pyproj import Transformer\n",
    "from pyproj.database import query_utm_crs_info\n",
    "from lib.functions import xarray_to_rasterio, mask_raster_with_geometry, create_memory_dataset, utm_from_extent, nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23da0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pystac.version.get_stac_version())\n",
    "print(pystac_client.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-inspection",
   "metadata": {},
   "source": [
    "# Get database data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-glucose",
   "metadata": {},
   "source": [
    "Setting some variables used in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDATE='2020-01-01'\n",
    "EDATE='2020-01-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-outside",
   "metadata": {},
   "source": [
    "Reading configuration file with info about the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.read(\"setting.ini\")\n",
    "dbsett = config[\"eurodeer_db\"]\n",
    "eu_bbox = [\"-24.2\",\"35.2\",\"43.4\",\"71.0\"]\n",
    "# create connection with eurodeer_db\n",
    "db_connection_url = \"postgresql://{us}:{pas}@{host}:{port}/{db}\".format(us=dbsett['user'],\n",
    "                                                                      pas=dbsett['password'],\n",
    "                                                                      host=dbsett['host'],\n",
    "                                                                      port=dbsett['port'],\n",
    "                                                                      db=dbsett[\"db\"]\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-redhead",
   "metadata": {},
   "source": [
    "We now create a GeoPandas object with all the study areas and their bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_engine(db_connection_url)\n",
    "eu_sql = \"select study_areas_id, study_name, CASE WHEN geom is not null THEN geom WHEN geom_vhf is not null THEN geom_vhf END as geom from main.study_areas\"\n",
    "eu_df = gpd.GeoDataFrame.from_postgis(eu_sql, con)\n",
    "eu_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-responsibility",
   "metadata": {},
   "source": [
    "But it is more useful to get only study areas within the start and end dates, set before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe75278",
   "metadata": {},
   "outputs": [],
   "source": [
    "indate_sql = \"select distinct(sa.*) from main.animals as ani, main.study_areas as sa, (select distinct(animals_id) from main.gps_data_animals as gps where acquisition_time >= '{SDATE}' and acquisition_time < '{EDATE}') as q where q.animals_id = ani.animals_id and ani.study_areas_id=sa.study_areas_id\".format(SDATE=SDATE, EDATE=EDATE)\n",
    "indate_df = gpd.GeoDataFrame.from_postgis(indate_sql, con)\n",
    "indate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_df = indate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-campbell",
   "metadata": {},
   "source": [
    "For the test we are going to use Cembra area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyarea = eu_df[eu_df[\"study_areas_id\"] == 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyarea.hvplot(tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-agent",
   "metadata": {},
   "source": [
    "And we get the bounding box of the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = studyarea.geom.envelope.bounds\n",
    "print(bbox.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-theorem",
   "metadata": {},
   "source": [
    "We get the EPSG code to use from the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSG = utm_from_extent(bbox.values[0])\n",
    "print(EPSG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-yukon",
   "metadata": {},
   "source": [
    "Now we print the study area in hectares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyarea_m2 = float(studyarea.to_crs(f\"EPSG:{EPSG.to_epsg()}\").geom.area)\n",
    "studyarea_ha = studyarea_m2 / 1000000\n",
    "print(studyarea_ha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-shoot",
   "metadata": {},
   "source": [
    "reproject the bbox to the corresponding UTM values; **pay attention that the newer Proj version for EPSG:4326 that y,x as input and return x,y as output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRONG since now 4326 is lat, long\n",
    "transformer = Transformer.from_crs(\"epsg:4326\", f'epsg:{EPSG.to_epsg()}')\n",
    "x1, y1 = transformer.transform(bbox.minx.values[0], bbox.miny.values[0])\n",
    "x2, y2 = transformer.transform(bbox.maxx.values[0], bbox.maxy.values[0])\n",
    "bbox_utm = (x1, y1, x2, y2)\n",
    "print(bbox_utm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = transformer.transform(bbox.miny.values[0], bbox.minx.values[0])\n",
    "x2, y2 = transformer.transform(bbox.maxy.values[0], bbox.maxx.values[0])\n",
    "bbox_utm_reverse = (x1, y1, x2, y2)\n",
    "print(bbox_utm_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-desire",
   "metadata": {},
   "source": [
    "Get the GPS point data for the selected times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_sql = \"select gps.* from main.gps_data_animals as gps where gps.animals_id in (select animals_id from main.animals where study_areas_id={na} and acquisition_time >= '{ts}' and acquisition_time < '{te}');\".format(na=studyarea.study_areas_id.values[0], ts=SDATE, te=EDATE)\n",
    "print(gps_sql)\n",
    "studyarea_points = gpd.GeoDataFrame.from_postgis(gps_sql, con, geom_col='geom')\n",
    "len(studyarea_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "#studyarea_points.geom.is_empty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#studyarea_points[~(studyarea_points.is_empty | studyarea_points.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(studyarea_points['acquisition_time']))\n",
    "print(max(studyarea_points['acquisition_time']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93686509",
   "metadata": {},
   "source": [
    "Create date range for the pystac Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "mindate = min(studyarea_points['acquisition_time']).date().strftime('%Y-%m-%d')\n",
    "maxdate = max(studyarea_points['acquisition_time']).date().strftime('%Y-%m-%d')\n",
    "date_range = \"{mi}/{ma}\".format(mi=mindate, ma=maxdate)\n",
    "print(date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32eff2c",
   "metadata": {},
   "source": [
    "Get all the dates where there are some data (useful to avoid days without data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyarea_points_dates = sorted(set(studyarea_points.acquisition_time.dt.strftime(\"%Y-%m-%d\").to_list()))\n",
    "studyarea_points_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-adams",
   "metadata": {},
   "source": [
    "## Working with STAC repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827089c9",
   "metadata": {},
   "source": [
    "The **SpatioTemporal Asset Catalog** (STAC) specification provides a common language to describe a range of geospatial information, so it can more easily be indexed and discovered. A 'spatiotemporal asset' is any file that represents information about the earth captured in a certain space and time. \n",
    "\n",
    "The goal is for all providers of spatiotemporal assets (Imagery, SAR, Point Clouds, Data Cubes, Full Motion Video, etc) to expose their data as SpatioTemporal Asset Catalogs (STAC), so that new code doesn't need to be written whenever a new data set or API is released.\n",
    "\n",
    "https://stacspec.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-touch",
   "metadata": {},
   "source": [
    "### Working with AWS storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-resolution",
   "metadata": {},
   "source": [
    "Inizialize the python STAC Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_catalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-vocabulary",
   "metadata": {},
   "source": [
    "Print all the available catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aws_catalog.get_all_collections():print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-catch",
   "metadata": {},
   "source": [
    "Search for tiles in Sentinel 2 L2A cogs dataset where the bounding box is contained in our study area and in the dates range used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ebd98",
   "metadata": {},
   "source": [
    "**Cloud Optimized GeoTIFF** (COGS) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing ​HTTP GET range requests to ask for just the parts of a file they need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_sent = aws_catalog.search(collections=[\"sentinel-s2-l2a-cogs\"], bbox=bbox.values[0],\n",
    "    datetime=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-lover",
   "metadata": {},
   "source": [
    "Get all the items founded and print the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_sent_items = aws_sent.get_all_items()\n",
    "print(len(aws_sent_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-writer",
   "metadata": {},
   "source": [
    "Let's see what is an item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=aws_sent_items.items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "item.assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-missile",
   "metadata": {},
   "source": [
    "We are going to create a STAC object with all the STAC items with only the band RED and NIR clipping the data in the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = stackstac.stack(aws_sent_items, assets=[\"B04\", \"B08\"], resolution=10, epsg=EPSG, bounds_latlon=bbox.values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-thunder",
   "metadata": {},
   "source": [
    "Calculate NDVI dataset and bring the attributes from original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir, red = ds.sel(band=\"B08\"), ds.sel(band=\"B04\")\n",
    "ndvi = (nir - red) / (nir + red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.attrs = ds.attrs\n",
    "ndvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-investment",
   "metadata": {},
   "source": [
    "Now we get the uniq dates for the NDVI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_days = set(sorted(list(ndvi.time.dt.date.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-grace",
   "metadata": {},
   "source": [
    "For each NDVI day we assign all the gps dates to be queried "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_dates = {}\n",
    "#we create first the dictionary\n",
    "for i in ndvi_days:\n",
    "    assigned_dates[i.strftime(\"%Y-%m-%d\")] = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and later we fill it using the nearest function\n",
    "for i in studyarea_points_dates:\n",
    "    idate = datetime.strptime(i, \"%Y-%m-%d\").date()\n",
    "    near = nearest(ndvi_days, idate)\n",
    "    assigned_dates[near.strftime(\"%Y-%m-%d\")].append(idate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-button",
   "metadata": {},
   "source": [
    "For each NDVI map date we process the data, creating the mosaic, if needed, and query the map with the points for the selected time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "test ={'2020-01-27': [date(2020, 1, 26),\n",
    "  date(2020, 1, 27),\n",
    "  date(2020, 1, 28)],\n",
    " '2020-01-29': [date(2020, 1, 29), date(2020, 1, 30)],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in test.items():\n",
    "    #get NDVI maps for the selected date\n",
    "    maps_merge = ndvi[ndvi.time.dt.date == datetime.strptime(k, \"%Y-%m-%d\").date()]\n",
    "    daily_raster = []\n",
    "    #with more maps will merge them \n",
    "    if len(maps_merge) > 1:\n",
    "        for inmerge in maps_merge:\n",
    "            daily_raster.append(create_memory_dataset(inmerge, EPSG, ndvi.attrs['transform']))\n",
    "        merged, mergedtransf = merge(daily_raster)\n",
    "        inmem = create_memory_dataset(merged, EPSG, mergedtransf)\n",
    "    else:\n",
    "        inmem = create_memory_dataset(maps_merge[0], EPSG, ndvi.attrs['transform'])\n",
    "    # get data points beetween minimum and maximum date connected to the date of NDVI map\n",
    "    print(type(v))\n",
    "    daypoints=studyarea_points[(studyarea_points.acquisition_time.dt.date >= min(v)) & (studyarea_points.acquisition_time.dt.date < max(v))]\n",
    "    daypointsxy = {}\n",
    "    # we need to transform the points from LatLong to UTM\n",
    "    for i,x,y in zip(daypoints.gps_data_animals_id, daypoints.geom.x , daypoints.geom.y):\n",
    "        daypointsxy[i] = transformer.transform(y,x)\n",
    "    x=0\n",
    "    keys = list(daypointsxy.keys())\n",
    "    #get points data from the raster\n",
    "    for val in inmem.sample(daypointsxy.values()):\n",
    "        if len(val) == 1:\n",
    "            print(keys[x], val[0])\n",
    "        else:\n",
    "            print(\"No data for ID {}\".format(key[x]))\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-bishop",
   "metadata": {},
   "source": [
    "It could be also possible to save the map to raster and use them in different program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in assigned_dates.keys():\n",
    "    maps_merge = ndvi[ndvi.time.dt.date == datetime.strptime(k, \"%Y-%m-%d\").date()]\n",
    "    daily_raster = []\n",
    "    #with more maps will merge them \n",
    "    if len(maps_merge) > 1:\n",
    "        for inmerge in maps_merge:\n",
    "            daily_raster.append(create_memory_dataset(inmerge, EPSG, ndvi.attrs['transform']))\n",
    "        out_image, out_transform = merge(daily_raster)\n",
    "    else:\n",
    "        out_image, out_transform = mask_raster_with_geometry(maps_merge[0],\n",
    "                                                             studyarea.to_crs(EPSG).geom,\n",
    "                                                             crop=True, nodata=np.nan)\n",
    "    out_meta = ds.attrs\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform,\n",
    "                     \"crs\": EPSG\n",
    "                    })\n",
    "    bandtime = maps_merge[0].time.dt.date.values\n",
    "    print(out_image.dtype)\n",
    "    \n",
    "    with rasterio.open(f\"/tmp/ndvi{bandtime}.tif\", \"w\", dtype=out_image.dtype, count=1, nodata=None, **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "        print(f\"/tmp/ndvi{bandtime}.tif saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74649fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_sent = aws_catalog.search(collections=[\"sentinel-s2-l2a-cogs\"], bbox=bbox.values[0],\n",
    "    datetime=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-palace",
   "metadata": {},
   "source": [
    "## Known problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-swimming",
   "metadata": {},
   "source": [
    "* **missing data**: clouds and not constant fligh area\n",
    "\n",
    "* **time**: it will take long time to process all the data, one of the most time consuming activity is the point query with raster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-structure",
   "metadata": {},
   "source": [
    "## Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-friend",
   "metadata": {},
   "source": [
    "* test other solution to query the points (passing by GRASS GIS for example)\n",
    "\n",
    "* extract a script from the above lines to be run in multiple process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-patrol",
   "metadata": {},
   "source": [
    "## Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-danish",
   "metadata": {},
   "source": [
    "* **Landsat** data to increse the number of maps and to reduce the probability to find clouds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
